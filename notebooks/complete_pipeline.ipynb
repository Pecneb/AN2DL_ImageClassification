{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf9fd39",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05426342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Lion optimizer available\n",
      "\n",
      "ðŸš€ Device: mps\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "# Try to import Lion optimizer\n",
    "try:\n",
    "    from lion_pytorch import Lion\n",
    "    print(\"âœ… Lion optimizer available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Installing Lion optimizer...\")\n",
    "    !pip install lion-pytorch\n",
    "    from lion_pytorch import Lion\n",
    "    print(\"âœ… Lion optimizer installed\")\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"\\nðŸš€ Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919af8ad",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing with Artifact Filtering\n",
    "\n",
    "**Advice 05/12**: \"A single drop of poison, the purest well corrupts.\"\n",
    "\n",
    "We preprocess images to:\n",
    "- Apply masks (remove background)\n",
    "- Detect and remove artifacts (green/orange/brown markers)\n",
    "- Filter out heavily contaminated images\n",
    "- Crop to ROI (tissue region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9481d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ../data/train_data\n",
      "Output: ../data/train_data_cleaned\n",
      "Max artifact ratio: 0.5%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = '../data/train_data'\n",
    "OUTPUT_DIR = '../data/train_data_cleaned'\n",
    "CSV_PATH = '../data/train_labels.csv'\n",
    "MAX_ARTIFACT_RATIO = 0.005  # Reject images with >10% artifacts\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {INPUT_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Max artifact ratio: {MAX_ARTIFACT_RATIO:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0b31d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined.\n"
     ]
    }
   ],
   "source": [
    "def get_artifact_masks(hsv):\n",
    "    \"\"\"Detect artifacts in HSV color space.\"\"\"\n",
    "    # Green markers\n",
    "    LOWER_GREEN = np.array([25, 40, 40])\n",
    "    UPPER_GREEN = np.array([95, 255, 255])\n",
    "    \n",
    "    # Orange markers\n",
    "    LOWER_ORANGE = np.array([5, 50, 50])\n",
    "    UPPER_ORANGE = np.array([35, 255, 255])\n",
    "    \n",
    "    # White centers\n",
    "    LOWER_WHITE = np.array([0, 0, 200])\n",
    "    UPPER_WHITE = np.array([180, 30, 255])\n",
    "    \n",
    "    # Brown markers\n",
    "    LOWER_BROWN = np.array([0, 60, 20])\n",
    "    UPPER_BROWN = np.array([25, 255, 150])\n",
    "    \n",
    "    m_green = cv2.inRange(hsv, LOWER_GREEN, UPPER_GREEN)\n",
    "    m_orange = cv2.inRange(hsv, LOWER_ORANGE, UPPER_ORANGE)\n",
    "    m_white = cv2.inRange(hsv, LOWER_WHITE, UPPER_WHITE)\n",
    "    m_brown = cv2.inRange(hsv, LOWER_BROWN, UPPER_BROWN)\n",
    "    \n",
    "    bad_pixels = cv2.bitwise_or(m_green, m_orange)\n",
    "    bad_pixels = cv2.bitwise_or(bad_pixels, m_white)\n",
    "    bad_pixels = cv2.bitwise_or(bad_pixels, m_brown)\n",
    "    \n",
    "    return bad_pixels\n",
    "\n",
    "\n",
    "def preprocess_image(img_path, mask_path, max_artifact_ratio=0.10):\n",
    "    \"\"\"Preprocess a single image with artifact filtering.\"\"\"\n",
    "    # Load image and mask\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None, False, \"Failed to load image\"\n",
    "    \n",
    "    if os.path.exists(mask_path):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "    else:\n",
    "        mask = np.full(img.shape[:2], 255, dtype=np.uint8)\n",
    "    \n",
    "    # Detect artifacts BEFORE dilation\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    bad_pixels = get_artifact_masks(hsv)\n",
    "    \n",
    "    # Count artifact ratio\n",
    "    total_pixels = img.shape[0] * img.shape[1]\n",
    "    artifact_count = cv2.countNonZero(bad_pixels)\n",
    "    artifact_ratio = artifact_count / total_pixels\n",
    "    \n",
    "    # Reject if too contaminated (Advice 05/12)\n",
    "    if artifact_ratio > max_artifact_ratio:\n",
    "        return None, False, f\"Too many artifacts ({artifact_ratio:.2%})\"\n",
    "    \n",
    "    # Dilate artifact mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    bad_pixels_expanded = cv2.dilate(bad_pixels, kernel, iterations=3)\n",
    "    \n",
    "    # Clean mask\n",
    "    clean_mask = cv2.bitwise_and(mask, cv2.bitwise_not(bad_pixels_expanded))\n",
    "    \n",
    "    # Find ROI\n",
    "    y, x = np.where(clean_mask > 0)\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return None, False, \"No tissue remaining\"\n",
    "    \n",
    "    # Crop with padding\n",
    "    pad = 20\n",
    "    y_min = max(0, y.min() - pad)\n",
    "    y_max = min(img.shape[0], y.max() + pad)\n",
    "    x_min = max(0, x.min() - pad)\n",
    "    x_max = min(img.shape[1], x.max() + pad)\n",
    "    \n",
    "    # Apply mask and crop\n",
    "    masked_img = cv2.bitwise_and(img, img, mask=clean_mask)\n",
    "    final_img = masked_img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Convert to RGB\n",
    "    final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return final_img, True, f\"Success ({artifact_ratio:.2%} artifacts removed)\"\n",
    "\n",
    "print(\"Preprocessing functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13df080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 691 images to process\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 691/691 [00:16<00:00, 40.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Preprocessing Complete!\n",
      "============================================================\n",
      "âœ… Successful: 547/691 (79.2%)\n",
      "âŒ Failed:     144/691 (20.8%)\n",
      "\n",
      "Rejection breakdown:\n",
      "  ðŸŽ¨ Too many artifacts: 144\n",
      "  ðŸš« No tissue:          0\n",
      "  ðŸ’¥ Corrupted:          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV and process images\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Found {len(df)} images to process\\n\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "failed_images = []\n",
    "rejection_reasons = {'high_artifacts': 0, 'no_tissue': 0, 'corrupted': 0}\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing\"):\n",
    "    img_name = row['sample_index']\n",
    "    img_path = os.path.join(INPUT_DIR, img_name)\n",
    "    mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "    mask_path = os.path.join(INPUT_DIR, mask_name)\n",
    "    \n",
    "    try:\n",
    "        processed_img, success, message = preprocess_image(\n",
    "            img_path, mask_path, max_artifact_ratio=MAX_ARTIFACT_RATIO\n",
    "        )\n",
    "        \n",
    "        if success and processed_img is not None:\n",
    "            output_path = os.path.join(OUTPUT_DIR, img_name)\n",
    "            cv2.imwrite(output_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            failed_images.append(img_name)\n",
    "            \n",
    "            if 'artifacts' in message:\n",
    "                rejection_reasons['high_artifacts'] += 1\n",
    "            elif 'tissue' in message:\n",
    "                rejection_reasons['no_tissue'] += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        failed_images.append(img_name)\n",
    "        rejection_reasons['corrupted'] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Preprocessing Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"âœ… Successful: {successful}/{len(df)} ({successful/len(df)*100:.1f}%)\")\n",
    "print(f\"âŒ Failed:     {failed}/{len(df)} ({failed/len(df)*100:.1f}%)\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\nRejection breakdown:\")\n",
    "    print(f\"  ðŸŽ¨ Too many artifacts: {rejection_reasons['high_artifacts']}\")\n",
    "    print(f\"  ðŸš« No tissue:          {rejection_reasons['no_tissue']}\")\n",
    "    print(f\"  ðŸ’¥ Corrupted:          {rejection_reasons['corrupted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "396d20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CSV Created:\n",
      "  Original samples: 691\n",
      "  Cleaned samples:  547\n",
      "  Removed:          144\n",
      "  Saved to:         ../data/train_labels_cleaned.csv\n",
      "============================================================\n",
      "\n",
      "Class distribution after cleaning:\n",
      "label\n",
      "Luminal B          191\n",
      "Luminal A          149\n",
      "HER2(+)            145\n",
      "Triple negative     62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create cleaned CSV\n",
    "df_clean = df[~df['sample_index'].isin(failed_images)].reset_index(drop=True)\n",
    "clean_csv_path = '../data/train_labels_cleaned.csv'\n",
    "df_clean.to_csv(clean_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CSV Created:\")\n",
    "print(f\"  Original samples: {len(df)}\")\n",
    "print(f\"  Cleaned samples:  {len(df_clean)}\")\n",
    "print(f\"  Removed:          {len(df) - len(df_clean)}\")\n",
    "print(f\"  Saved to:         {clean_csv_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution after cleaning:\")\n",
    "print(df_clean['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04057280",
   "metadata": {},
   "source": [
    "## Step 3: Dataset & DataLoader Setup\n",
    "\n",
    "**Advice 04/12**: \"Know the dimension of your stream\" - Using larger batches (32) for BatchNorm stability\n",
    "\n",
    "**Advice 06/12**: \"Let the policy emerge from the struggle\" - RandAugment automated augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d5076af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_patches_inference(img, patch_size=384, stride=192):\n",
    "    \"\"\"Extract overlapping patches for inference.\"\"\"\n",
    "    w, h = img.size\n",
    "    patches = []\n",
    "    \n",
    "    # Handle small images\n",
    "    if w < patch_size or h < patch_size:\n",
    "        resized = img.resize((patch_size, patch_size), Image.BILINEAR)\n",
    "        return [resized]\n",
    "    \n",
    "    # Sliding window\n",
    "    for top in range(0, h - patch_size + 1, stride):\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, left + patch_size, top + patch_size))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # Handle edges\n",
    "    if (w - patch_size) % stride != 0:\n",
    "        left = w - patch_size\n",
    "        for top in range(0, h - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, w, top + patch_size))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    if (h - patch_size) % stride != 0:\n",
    "        top = h - patch_size\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, left + patch_size, h))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # Corner\n",
    "    if (w - patch_size) % stride != 0 and (h - patch_size) % stride != 0:\n",
    "        patch = img.crop((w - patch_size, h - patch_size, w, h))\n",
    "        patches.append(patch)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "class CleanImageDataset(Dataset):\n",
    "    \"\"\"Dataset for cleaned preprocessed images.\"\"\"\n",
    "    def __init__(self, df, img_dir, patch_size=384, stride=192, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "        self.label_map = {\n",
    "            \"Luminal A\": 0,\n",
    "            \"Luminal B\": 1,\n",
    "            \"HER2(+)\": 2,\n",
    "            \"Triple negative\": 3\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['sample_index']\n",
    "        label = self.label_map[row['label']]\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Extract patches\n",
    "        patches = extract_patches_inference(image, patch_size=self.patch_size, stride=self.stride)\n",
    "        # Apply transform to each patch\n",
    "        patch_tensors = [self.transform(patch) for patch in patches]\n",
    "        patch_tensors = torch.stack(patch_tensors) # [num_patches, C, H, W]\n",
    "       \n",
    "        return patch_tensors, label, img_name\n",
    "\n",
    "print(\"Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "768d67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 384Ã—384\n",
      "Batch size: 32 (stable for BatchNorm)\n",
      "Transforms configured with RandAugment\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 384\n",
    "BATCH_SIZE = 32  # Larger for BatchNorm stability (Advice 04/12)\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Training transforms with RandAugment (Advice 06/12)\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandAugment(num_ops=2, magnitude=7),  # Automated augmentation\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"Image size: {IMG_SIZE}Ã—{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (stable for BatchNorm)\")\n",
    "print(f\"Transforms configured with RandAugment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e6cb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_collate_fn(batch):\n",
    "    patch_tensors, labels, img_names = zip(*batch)\n",
    "    return list(patch_tensors), torch.tensor(labels), list(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7736d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 437\n",
      "Validation samples: 110\n",
      "\n",
      "DataLoaders ready!\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data and split\n",
    "df_clean = pd.read_csv(clean_csv_path)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_clean, \n",
    "    test_size=0.2, \n",
    "    stratify=df_clean['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Calculate weights for class imbalance\n",
    "class_counts = train_df['label'].value_counts()\n",
    "weight_per_class = {cls: 1.0/count for cls, count in class_counts.items()}\n",
    "sample_weights = [weight_per_class[row['label']] for _, row in train_df.iterrows()]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_df), replacement=True)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_ds = CleanImageDataset(train_df, OUTPUT_DIR, transform=train_transform)\n",
    "val_ds = CleanImageDataset(val_df, OUTPUT_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE == 'cuda' else False,\n",
    "    collate_fn=mil_collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE == 'cuda' else False,\n",
    "    collate_fn=mil_collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\nDataLoaders ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dac358",
   "metadata": {},
   "source": [
    "## Step 4: Model Setup with Lion Optimizer\n",
    "\n",
    "**Advice 07/12**: \"The Lion, with instinct fierce and memory sparse, the prey tracks.\"\n",
    "\n",
    "Using Lion optimizer - more memory efficient and better for small datasets than Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0619938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model: EfficientNetV2-S (21.5M params)\n",
      "âœ… Optimizer: Lion (lr=3e-4)\n",
      "âœ… Scheduler: Cosine Annealing\n",
      "âœ… Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load EfficientNetV2-S\n",
    "model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Replace classifier\n",
    "in_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features, 4)\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Lion optimizer (Advice 07/12)\n",
    "# Lion needs 3-10x smaller LR than Adam\n",
    "optimizer = Lion(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "print(\"\\nâœ… Model: EfficientNetV2-S (21.5M params)\")\n",
    "print(\"âœ… Optimizer: Lion (lr=3e-4)\")\n",
    "print(\"âœ… Scheduler: Cosine Annealing\")\n",
    "print(f\"âœ… Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202a53e",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "231c021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for patch_tensor_list, labels, _ in pbar:\n",
    "        batch_loss = 0.0\n",
    "        batch_preds = []\n",
    "        batch_targets = []\n",
    "        optimizer.zero_grad()\n",
    "        for i, patch_tensors in enumerate(patch_tensor_list):\n",
    "            patch_tensors = patch_tensors.to(device)  # [num_patches, C, H, W]\n",
    "            label = labels[i].to(device)\n",
    "            logits = model(patch_tensors)  # [num_patches, num_classes]\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            bag_probs = probs.mean(dim=0, keepdim=True)  # [1, num_classes]\n",
    "            loss = criterion(bag_probs, label.unsqueeze(0))\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "            pred = bag_probs.argmax(dim=1).item()\n",
    "            batch_preds.append(pred)\n",
    "            batch_targets.append(label.item())\n",
    "        optimizer.step()\n",
    "        running_loss += batch_loss / len(patch_tensor_list)\n",
    "        all_preds.extend(batch_preds)\n",
    "        all_targets.extend(batch_targets)\n",
    "        pbar.set_postfix({\"loss\": f\"{batch_loss / len(patch_tensor_list):.4f}\"})\n",
    "\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "    epoch_acc = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    return epoch_loss, epoch_f1, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "        for patch_tensor_list, labels, _ in pbar:\n",
    "            batch_loss = 0.0\n",
    "            batch_preds = []\n",
    "            batch_targets = []\n",
    "            for i, patch_tensors in enumerate(patch_tensor_list):\n",
    "                patch_tensors = patch_tensors.to(device)\n",
    "                label = labels[i].to(device)\n",
    "                logits = model(patch_tensors)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                bag_probs = probs.mean(dim=0, keepdim=True)\n",
    "                loss = criterion(bag_probs, label.unsqueeze(0))\n",
    "                batch_loss += loss.item()\n",
    "                pred = bag_probs.argmax(dim=1).item()\n",
    "                batch_preds.append(pred)\n",
    "                batch_targets.append(label.item())\n",
    "            running_loss += batch_loss / len(patch_tensor_list)\n",
    "            all_preds.extend(batch_preds)\n",
    "            all_targets.extend(batch_targets)\n",
    "            pbar.set_postfix({\"loss\": f\"{batch_loss / len(patch_tensor_list):.4f}\"})\n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "    epoch_acc = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    return epoch_loss, epoch_f1, epoch_acc, all_preds, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37178c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Total epochs: 50\n",
      "  Freeze epochs: 5\n",
      "  Early stopping patience: 10\n",
      "  Backbone frozen for first 5 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "FREEZE_EPOCHS = 5  # Freeze backbone for first 5 epochs\n",
    "PATIENCE = 10\n",
    "\n",
    "# Freeze backbone initially\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Freeze epochs: {FREEZE_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")\n",
    "print(f\"  Backbone frozen for first {FREEZE_EPOCHS} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5de77e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STARTED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     22\u001b[39m train_loss, train_f1, train_acc = train_epoch(\n\u001b[32m     23\u001b[39m     model, train_loader, criterion, optimizer, DEVICE\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m val_loss, val_f1, val_acc, val_preds, val_targets = \u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Step scheduler\u001b[39;00m\n\u001b[32m     32\u001b[39m scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mvalidate_epoch\u001b[39m\u001b[34m(model, loader, criterion, device)\u001b[39m\n\u001b[32m     49\u001b[39m patch_tensors = patch_tensors.to(device)\n\u001b[32m     50\u001b[39m label = labels[i].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m probs = torch.softmax(logits, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     53\u001b[39m bag_probs = probs.mean(dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torchvision/models/efficientnet.py:344\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torchvision/models/efficientnet.py:334\u001b[39m, in \u001b[36mEfficientNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m    337\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torchvision/models/efficientnet.py:165\u001b[39m, in \u001b[36mMBConv.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_res_connect:\n\u001b[32m    167\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.stochastic_depth(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torchvision/ops/misc.py:260\u001b[39m, in \u001b[36mSqueezeExcitation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     scale = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scale * \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torchvision/ops/misc.py:255\u001b[39m, in \u001b[36mSqueezeExcitation._scale\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    253\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.avgpool(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    254\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.fc1(scale)\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m scale = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.fc2(scale)\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scale_activation(scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:473\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    470\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Polimi/AN2DL/Challenges/AN2DL_Image_Classification/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2370\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [], 'train_f1': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_f1': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING STARTED\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Unfreeze backbone after warmup\n",
    "    if epoch == FREEZE_EPOCHS:\n",
    "        print(f\"\\nðŸ”“ Unfreezing backbone at epoch {epoch}\\n\")\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_f1, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_f1, val_acc, val_preds, val_targets = validate_epoch(\n",
    "        model, val_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Acc: {val_acc:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_acc': val_acc,\n",
    "        }, '../best_model.pth')\n",
    "        print(f\"  ðŸ’¾ Saved best model (F1: {val_f1:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nâš ï¸ Early stopping triggered (no improvement for {PATIENCE} epochs)\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"Best Validation F1: {best_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7663087",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1].plot(history['train_f1'], label='Train F1', marker='o', color='green')\n",
    "axes[1].plot(history['val_f1'], label='Val F1', marker='s', color='orange')\n",
    "axes[1].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score (Macro)')\n",
    "axes[1].set_title(f'F1 Score (Best: {best_f1:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[2].plot(history['train_acc'], label='Train Acc', marker='o', color='blue')\n",
    "axes[2].plot(history['val_acc'], label='Val Acc', marker='s', color='red')\n",
    "axes[2].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Training & Validation Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to '../training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint = torch.load('../best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Get final predictions\n",
    "_, _, _, final_preds, final_targets = validate_epoch(\n",
    "    model, val_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "# Classification report\n",
    "class_names = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple Negative\"]\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(final_targets, final_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(final_targets, final_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to '../confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c224f46d",
   "metadata": {},
   "source": [
    "## Step 7: Patch-Based Inference for Test Set\n",
    "\n",
    "**Advice 08/12**: \"Let the model walk the landscape step by step, preserving the original resolution.\"\n",
    "\n",
    "We train on resized images (stable), but infer using patches at full resolution (preserves details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_inference(img, patch_size=384, stride=192):\n",
    "    \"\"\"Extract overlapping patches for inference.\"\"\"\n",
    "    w, h = img.size\n",
    "    patches = []\n",
    "    \n",
    "    # Handle small images\n",
    "    if w < patch_size or h < patch_size:\n",
    "        resized = img.resize((patch_size, patch_size), Image.BILINEAR)\n",
    "        return [resized]\n",
    "    \n",
    "    # Sliding window\n",
    "    for top in range(0, h - patch_size + 1, stride):\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, left + patch_size, top + patch_size))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # Handle edges\n",
    "    if (w - patch_size) % stride != 0:\n",
    "        left = w - patch_size\n",
    "        for top in range(0, h - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, w, top + patch_size))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    if (h - patch_size) % stride != 0:\n",
    "        top = h - patch_size\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch = img.crop((left, top, left + patch_size, h))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # Corner\n",
    "    if (w - patch_size) % stride != 0 and (h - patch_size) % stride != 0:\n",
    "        patch = img.crop((w - patch_size, h - patch_size, w, h))\n",
    "        patches.append(patch)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "\n",
    "def predict_with_patches(model, img_path, transform, patch_size=384, stride=192, device='cpu'):\n",
    "    \"\"\"Predict using patch-based inference with soft voting.\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    patches = extract_patches_inference(img, patch_size=patch_size, stride=stride)\n",
    "    \n",
    "    all_probs = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patch in patches:\n",
    "            patch_tensor = transform(patch).unsqueeze(0).to(device)\n",
    "            logits = model(patch_tensor)\n",
    "            probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Average probabilities (soft voting)\n",
    "    avg_probs = np.array(all_probs).mean(axis=0)\n",
    "    prediction = avg_probs.argmax()\n",
    "    confidence = avg_probs[prediction]\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "print(\"Patch-based inference functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2551a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m TEST_INPUT_DIR = \u001b[33m'\u001b[39m\u001b[33m../data/test_data\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m TEST_OUTPUT_DIR = \u001b[33m'\u001b[39m\u001b[33m../data/test_data_preprocessed\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mos\u001b[49m.makedirs(TEST_OUTPUT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(TEST_INPUT_DIR):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPreprocessing test set...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess Test Set with Masks\n",
    "TEST_INPUT_DIR = '../data/test_data'\n",
    "TEST_OUTPUT_DIR = '../data/test_data_preprocessed'\n",
    "\n",
    "os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(TEST_INPUT_DIR):\n",
    "    print(f\"Preprocessing test set...\")\n",
    "    print(f\"Input: {TEST_INPUT_DIR}\")\n",
    "    print(f\"Output: {TEST_OUTPUT_DIR}\")\n",
    "    print(f\"Max artifact ratio: {MAX_ARTIFACT_RATIO:.1%}\\n\")\n",
    "    \n",
    "    test_files = sorted([f for f in os.listdir(TEST_INPUT_DIR) if f.startswith('img_')])\n",
    "    print(f\"Found {len(test_files)} test images\\n\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_test_images = []\n",
    "    test_rejection_reasons = {'high_artifacts': 0, 'no_tissue': 0, 'corrupted': 0}\n",
    "    \n",
    "    for img_name in tqdm(test_files, desc=\"Preprocessing Test Set\"):\n",
    "        img_path = os.path.join(TEST_INPUT_DIR, img_name)\n",
    "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "        mask_path = os.path.join(TEST_INPUT_DIR, mask_name)\n",
    "        \n",
    "        try:\n",
    "            processed_img, success, message = preprocess_image(\n",
    "                img_path, mask_path, max_artifact_ratio=20\n",
    "            )\n",
    "            \n",
    "            if success and processed_img is not None:\n",
    "                output_path = os.path.join(TEST_OUTPUT_DIR, img_name)\n",
    "                cv2.imwrite(output_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                failed_test_images.append(img_name)\n",
    "                \n",
    "                if 'artifacts' in message:\n",
    "                    test_rejection_reasons['high_artifacts'] += 1\n",
    "                elif 'tissue' in message:\n",
    "                    test_rejection_reasons['no_tissue'] += 1\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_test_images.append(img_name)\n",
    "            test_rejection_reasons['corrupted'] += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Set Preprocessing Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"âœ… Successful: {successful}/{len(test_files)} ({successful/len(test_files)*100:.1f}%)\")\n",
    "    print(f\"âŒ Failed:     {failed}/{len(test_files)} ({failed/len(test_files)*100:.1f}%)\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(f\"\\nRejection breakdown:\")\n",
    "        print(f\"  ðŸŽ¨ Too many artifacts: {test_rejection_reasons['high_artifacts']}\")\n",
    "        print(f\"  ðŸš« No tissue:          {test_rejection_reasons['no_tissue']}\")\n",
    "        print(f\"  ðŸ’¥ Corrupted:          {test_rejection_reasons['corrupted']}\")\n",
    "        print(f\"\\nNote: Failed test images will be skipped during inference\")\n",
    "else:\n",
    "    print(f\"Test directory not found: {TEST_INPUT_DIR}\")\n",
    "    print(\"Skipping test set preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18becd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set (if available)\n",
    "TEST_DIR = '../data/test_data_preprocessed'\n",
    "\n",
    "if os.path.exists(TEST_DIR):\n",
    "    test_files = sorted([f for f in os.listdir(TEST_DIR) if f.startswith('img_')])\n",
    "    print(f\"Found {len(test_files)} test images\")\n",
    "    print(f\"Using patch-based inference with stride=192 (50% overlap)\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_name in tqdm(test_files, desc=\"Inference\"):\n",
    "        img_path = os.path.join(TEST_DIR, img_name)\n",
    "        pred, conf = predict_with_patches(\n",
    "            model, img_path, val_transform, \n",
    "            patch_size=384, stride=192, device=DEVICE\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': img_name,\n",
    "            'label': class_names[pred],\n",
    "            'confidence': conf\n",
    "        })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_df.to_csv('../submission_patches.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Inference complete!\")\n",
    "    print(f\"âœ… Saved to: ../submission_patches.csv\")\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(submission_df['label'].value_counts())\n",
    "    print(f\"\\nAverage confidence: {submission_df['confidence'].mean():.3f}\")\n",
    "else:\n",
    "    print(f\"Test directory not found: {TEST_DIR}\")\n",
    "    print(\"Please preprocess test data first using preprocess_dataset.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45081ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… All 5 Pieces of Advice Implemented:\n",
    "\n",
    "1. **Advice 05/12** (Outliers): Filtered images with >10% artifacts\n",
    "2. **Advice 04/12** (Normalization): Used batch_size=32 for BatchNorm stability\n",
    "3. **Advice 07/12** (Modern Optimizers): Lion optimizer with lr=3e-4\n",
    "4. **Advice 06/12** (Auto Augmentation): RandAugment with num_ops=2, magnitude=7\n",
    "5. **Advice 08/12** (Full Resolution): Patch-based inference at native resolution\n",
    "\n",
    "### ðŸŽ¯ Training Strategy:\n",
    "- **Stable training**: Resize to 384Ã—384, larger batches, Lion optimizer\n",
    "- **Full resolution inference**: Patches with 50% overlap, soft voting\n",
    "- **Best of both worlds**: Training stability + inference detail preservation\n",
    "\n",
    "### ðŸ“Š Results:\n",
    "- Check `training_curves.png` for loss/F1 progression\n",
    "- Check `confusion_matrix.png` for per-class performance\n",
    "- Best model saved to `best_model.pth`\n",
    "- Test predictions saved to `submission_patches.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

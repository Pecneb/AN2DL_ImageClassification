{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28540c4e",
   "metadata": {},
   "source": [
    "## Step 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a265f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lion optimizer available\n",
      "\n",
      "üöÄ Device: mps\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "\n",
    "# Try to import Lion optimizer\n",
    "try:\n",
    "    from lion_pytorch import Lion\n",
    "    print(\"‚úÖ Lion optimizer available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing Lion optimizer...\")\n",
    "    !pip install lion-pytorch\n",
    "    from lion_pytorch import Lion\n",
    "    print(\"‚úÖ Lion optimizer installed\")\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"\\nüöÄ Device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49da4d6",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing with Artifact Filtering\n",
    "\n",
    "**Advice 05/12**: \"A single drop of poison, the purest well corrupts.\"\n",
    "\n",
    "We preprocess images to:\n",
    "- Apply masks (remove background)\n",
    "- Detect and remove artifacts (green/orange/brown markers)\n",
    "- Filter out heavily contaminated images\n",
    "- Crop to ROI (tissue region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bad2c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ../data/train_data\n",
      "Output: ../data/train_data_cleaned\n",
      "Max artifact ratio: 0.5%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = '../data/train_data'\n",
    "OUTPUT_DIR = '../data/train_data_cleaned'\n",
    "CSV_PATH = '../data/train_labels.csv'\n",
    "MAX_ARTIFACT_RATIO = 0.005  # Reject images with >10% artifacts\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {INPUT_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Max artifact ratio: {MAX_ARTIFACT_RATIO:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80d02e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined (now returns both image and mask).\n"
     ]
    }
   ],
   "source": [
    "def get_artifact_masks(hsv):\n",
    "    \"\"\"Detect artifacts in HSV color space.\"\"\"\n",
    "    # Green markers\n",
    "    LOWER_GREEN = np.array([25, 40, 40])\n",
    "    UPPER_GREEN = np.array([95, 255, 255])\n",
    "    \n",
    "    # Orange markers\n",
    "    LOWER_ORANGE = np.array([5, 50, 50])\n",
    "    UPPER_ORANGE = np.array([35, 255, 255])\n",
    "    \n",
    "    # White centers\n",
    "    LOWER_WHITE = np.array([0, 0, 200])\n",
    "    UPPER_WHITE = np.array([180, 30, 255])\n",
    "    \n",
    "    # Brown markers\n",
    "    LOWER_BROWN = np.array([0, 60, 20])\n",
    "    UPPER_BROWN = np.array([25, 255, 150])\n",
    "    \n",
    "    m_green = cv2.inRange(hsv, LOWER_GREEN, UPPER_GREEN)\n",
    "    m_orange = cv2.inRange(hsv, LOWER_ORANGE, UPPER_ORANGE)\n",
    "    m_white = cv2.inRange(hsv, LOWER_WHITE, UPPER_WHITE)\n",
    "    m_brown = cv2.inRange(hsv, LOWER_BROWN, UPPER_BROWN)\n",
    "    \n",
    "    bad_pixels = cv2.bitwise_or(m_green, m_orange)\n",
    "    bad_pixels = cv2.bitwise_or(bad_pixels, m_white)\n",
    "    bad_pixels = cv2.bitwise_or(bad_pixels, m_brown)\n",
    "    \n",
    "    return bad_pixels\n",
    "\n",
    "\n",
    "def preprocess_image(img_path, mask_path, max_artifact_ratio=0.10):\n",
    "    \"\"\"Preprocess a single image with artifact filtering.\"\"\"\n",
    "    # Load image and mask\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None, None, False, \"Failed to load image\"\n",
    "    \n",
    "    if os.path.exists(mask_path):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "    else:\n",
    "        mask = np.full(img.shape[:2], 255, dtype=np.uint8)\n",
    "    \n",
    "    # Detect artifacts BEFORE dilation\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    bad_pixels = get_artifact_masks(hsv)\n",
    "    \n",
    "    # Count artifact ratio\n",
    "    total_pixels = img.shape[0] * img.shape[1]\n",
    "    artifact_count = cv2.countNonZero(bad_pixels)\n",
    "    artifact_ratio = artifact_count / total_pixels\n",
    "    \n",
    "    # Reject if too contaminated (Advice 05/12)\n",
    "    if artifact_ratio > max_artifact_ratio:\n",
    "        return None, None, False, f\"Too many artifacts ({artifact_ratio:.2%})\"\n",
    "    \n",
    "    # Dilate artifact mask\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    bad_pixels_expanded = cv2.dilate(bad_pixels, kernel, iterations=3)\n",
    "    \n",
    "    # Clean mask\n",
    "    clean_mask = cv2.bitwise_and(mask, cv2.bitwise_not(bad_pixels_expanded))\n",
    "    \n",
    "    # Find ROI\n",
    "    y, x = np.where(clean_mask > 0)\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return None, None, False, \"No tissue remaining\"\n",
    "    \n",
    "    # Crop with padding\n",
    "    pad = 20\n",
    "    y_min = max(0, y.min() - pad)\n",
    "    y_max = min(img.shape[0], y.max() + pad)\n",
    "    x_min = max(0, x.min() - pad)\n",
    "    x_max = min(img.shape[1], x.max() + pad)\n",
    "    \n",
    "    # Apply mask and crop\n",
    "    masked_img = cv2.bitwise_and(img, img, mask=clean_mask)\n",
    "    final_img = masked_img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Crop mask too\n",
    "    final_mask = clean_mask[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Convert to RGB\n",
    "    final_img = cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return final_img, final_mask, True, f\"Success ({artifact_ratio:.2%} artifacts removed)\"\n",
    "\n",
    "print(\"Preprocessing functions defined (now returns both image and mask).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77b6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 691 images to process\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 691/691 [00:19<00:00, 35.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Preprocessing Complete!\n",
      "============================================================\n",
      "‚úÖ Successful: 547/691 (79.2%)\n",
      "‚ùå Failed:     144/691 (20.8%)\n",
      "\n",
      "Rejection breakdown:\n",
      "  üé® Too many artifacts: 144\n",
      "  üö´ No tissue:          0\n",
      "  üí• Corrupted:          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV and process images\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Found {len(df)} images to process\\n\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "failed_images = []\n",
    "rejection_reasons = {'high_artifacts': 0, 'no_tissue': 0, 'corrupted': 0}\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing\"):\n",
    "    img_name = row['sample_index']\n",
    "    img_path = os.path.join(INPUT_DIR, img_name)\n",
    "    mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "    mask_path = os.path.join(INPUT_DIR, mask_name)\n",
    "    \n",
    "    try:\n",
    "        processed_img, processed_mask, success, message = preprocess_image(\n",
    "            img_path, mask_path, max_artifact_ratio=MAX_ARTIFACT_RATIO\n",
    "        )\n",
    "        \n",
    "        if success and processed_img is not None:\n",
    "            # Save both image and mask\n",
    "            img_output_path = os.path.join(OUTPUT_DIR, img_name)\n",
    "            mask_output_path = os.path.join(OUTPUT_DIR, mask_name)\n",
    "            cv2.imwrite(img_output_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(mask_output_path, processed_mask)\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            failed_images.append(img_name)\n",
    "            \n",
    "            if 'artifacts' in message:\n",
    "                rejection_reasons['high_artifacts'] += 1\n",
    "            elif 'tissue' in message:\n",
    "                rejection_reasons['no_tissue'] += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        failed_images.append(img_name)\n",
    "        rejection_reasons['corrupted'] += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Preprocessing Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚úÖ Successful: {successful}/{len(df)} ({successful/len(df)*100:.1f}%)\")\n",
    "print(f\"‚ùå Failed:     {failed}/{len(df)} ({failed/len(df)*100:.1f}%)\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(f\"\\nRejection breakdown:\")\n",
    "    print(f\"  üé® Too many artifacts: {rejection_reasons['high_artifacts']}\")\n",
    "    print(f\"  üö´ No tissue:          {rejection_reasons['no_tissue']}\")\n",
    "    print(f\"  üí• Corrupted:          {rejection_reasons['corrupted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce0dcf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CSV Created:\n",
      "  Original samples: 691\n",
      "  Cleaned samples:  547\n",
      "  Removed:          144\n",
      "  Saved to:         ../data/train_labels_cleaned_dual_branch.csv\n",
      "============================================================\n",
      "\n",
      "Class distribution after cleaning:\n",
      "label\n",
      "Luminal B          191\n",
      "Luminal A          149\n",
      "HER2(+)            145\n",
      "Triple negative     62\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create cleaned CSV\n",
    "df_clean = df[~df['sample_index'].isin(failed_images)].reset_index(drop=True)\n",
    "clean_csv_path = '../data/train_labels_cleaned_dual_branch.csv'\n",
    "df_clean.to_csv(clean_csv_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CSV Created:\")\n",
    "print(f\"  Original samples: {len(df)}\")\n",
    "print(f\"  Cleaned samples:  {len(df_clean)}\")\n",
    "print(f\"  Removed:          {len(df) - len(df_clean)}\")\n",
    "print(f\"  Saved to:         {clean_csv_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution after cleaning:\")\n",
    "print(df_clean['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935894d",
   "metadata": {},
   "source": [
    "## Step 3: Dataset & DataLoader Setup\n",
    "\n",
    "**Advice 04/12**: \"Know the dimension of your stream\" - Using larger batches (32) for BatchNorm stability\n",
    "\n",
    "**Advice 06/12**: \"Let the policy emerge from the struggle\" - RandAugment automated augmentation\n",
    "\n",
    "**Advice 10/12**: \"Parallel Paths\" - Dual-branch dataset loads both RGB image and binary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88d9867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual-branch dataset class defined.\n"
     ]
    }
   ],
   "source": [
    "class DualBranchDataset(Dataset):\n",
    "    \"\"\"Dataset for dual-branch model: loads both RGB image and binary mask.\"\"\"\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {\n",
    "            \"Luminal A\": 0,\n",
    "            \"Luminal B\": 1,\n",
    "            \"HER2(+)\": 2,\n",
    "            \"Triple negative\": 3\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['sample_index']\n",
    "        label = self.label_map[row['label']]\n",
    "        \n",
    "        # Load RGB image\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load binary mask\n",
    "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "        mask_path = os.path.join(self.img_dir, mask_name)\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply transform to RGB image (with normalization)\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            # Apply geometric transforms to mask but skip normalization\n",
    "            # Create mask transform without Normalize\n",
    "            mask_transform = T.Compose([\n",
    "                t for t in self.transform.transforms \n",
    "                if not isinstance(t, T.Normalize)\n",
    "            ])\n",
    "            mask = mask_transform(mask)\n",
    "            \n",
    "        return image, mask, label, img_name\n",
    "\n",
    "print(\"Dual-branch dataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eba4d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 384√ó384 (EfficientNetV2-S standard)\n",
      "Batch size: 32 (stable for BatchNorm)\n",
      "Transforms configured with RandAugment\n",
      "Dual-branch: RGB + Mask inputs\n"
     ]
    }
   ],
   "source": [
    "# Configuration - EfficientNet uses 384√ó384\n",
    "IMG_SIZE = 384  # EfficientNetV2-S standard input size\n",
    "BATCH_SIZE = 32  # Larger for BatchNorm stability (Advice 04/12)\n",
    "NUM_WORKERS = 0  # Must be 0 for Jupyter notebooks on macOS\n",
    "\n",
    "# Training transforms with RandAugment (Advice 06/12)\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandAugment(num_ops=2, magnitude=7),  # Automated augmentation\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"Image size: {IMG_SIZE}√ó{IMG_SIZE} (EfficientNetV2-S standard)\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (stable for BatchNorm)\")\n",
    "print(f\"Transforms configured with RandAugment\")\n",
    "print(f\"Dual-branch: RGB + Mask inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a731d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual-branch dataset class defined.\n",
      "Training samples: 437\n",
      "Validation samples: 110\n",
      "\n",
      "Dual-branch DataLoaders ready!\n"
     ]
    }
   ],
   "source": [
    "class DualBranchDataset(Dataset):\n",
    "    \"\"\"Dataset for dual-branch model: loads both RGB image and binary mask.\"\"\"\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_map = {\n",
    "            \"Luminal A\": 0,\n",
    "            \"Luminal B\": 1,\n",
    "            \"HER2(+)\": 2,\n",
    "            \"Triple negative\": 3\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['sample_index']\n",
    "        label = self.label_map[row['label']]\n",
    "        \n",
    "        # Load RGB image\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load binary mask\n",
    "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "        mask_path = os.path.join(self.img_dir, mask_name)\n",
    "        mask = Image.open(mask_path).convert('L')  # Grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            # Apply transform to RGB image (with normalization)\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            # Apply geometric transforms to mask but skip normalization\n",
    "            # Create mask transform without Normalize\n",
    "            mask_transform = T.Compose([\n",
    "                t for t in self.transform.transforms \n",
    "                if not isinstance(t, T.Normalize)\n",
    "            ])\n",
    "            mask = mask_transform(mask)\n",
    "            if mask.shape[0] == 1:\n",
    "                mask = mask.repeat(3, 1, 1)\n",
    "            \n",
    "        return image, mask, label, img_name\n",
    "\n",
    "print(\"Dual-branch dataset class defined.\")\n",
    "# Load cleaned data and split\n",
    "df_clean = pd.read_csv(clean_csv_path)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df_clean, \n",
    "    test_size=0.2, \n",
    "    stratify=df_clean['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Calculate weights for class imbalance\n",
    "class_counts = train_df['label'].value_counts()\n",
    "weight_per_class = {cls: 1.0/count for cls, count in class_counts.items()}\n",
    "sample_weights = [weight_per_class[row['label']] for _, row in train_df.iterrows()]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_df), replacement=True)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_ds = DualBranchDataset(train_df, OUTPUT_DIR, transform=train_transform)\n",
    "val_ds = DualBranchDataset(val_df, OUTPUT_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=sampler, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if DEVICE == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(\"\\nDual-branch DataLoaders ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18a49fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 384, 384])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[5][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90693fa2",
   "metadata": {},
   "source": [
    "## Step 4: Dual-Branch EfficientNetV2-S Model Setup\n",
    "\n",
    "**Advice 07/12**: \"The Lion, with instinct fierce and memory sparse, the prey tracks.\"\n",
    "\n",
    "**Advice 10/12**: \"Parallel Paths\" - Two branches run separately, meet at fusion\n",
    "\n",
    "**Dual-Branch EfficientNetV2-S**: \n",
    "- **RGB Branch**: Processes color, texture, cellular morphology (pretrained)\n",
    "- **Mask Branch**: Processes shape, boundaries, spatial structure (from scratch)\n",
    "- **Fusion**: Concatenate features before classification\n",
    "- Total parameters: ~42M (21M + 21M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "296e0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB branch features: 1280\n",
      "Mask branch features: 1280\n",
      "Total fused features: 2560\n",
      "\n",
      "‚úÖ Model: Dual-Branch EfficientNetV2-S\n",
      "‚úÖ RGB Branch: Pretrained on ImageNet\n",
      "‚úÖ Mask Branch: Train from scratch\n",
      "‚úÖ Fusion: Late concatenation\n",
      "‚úÖ Optimizer: Lion (lr=3e-4)\n",
      "‚úÖ Scheduler: Cosine Annealing\n",
      "‚úÖ Device: mps\n"
     ]
    }
   ],
   "source": [
    "class DualBranchEfficientNet(nn.Module):\n",
    "    \"\"\"Dual-branch EfficientNetV2-S: RGB + Mask branches with late fusion.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # RGB Branch - pretrained on ImageNet\n",
    "        self.rgb_branch = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "        rgb_features = self.rgb_branch.classifier[1].in_features\n",
    "        self.rgb_branch.classifier = nn.Identity()  # Remove classifier\n",
    "        \n",
    "        # Mask Branch - train from scratch (binary masks)\n",
    "        self.mask_branch = efficientnet_v2_s(weights=None)\n",
    "        mask_features = self.mask_branch.classifier[1].in_features\n",
    "        self.mask_branch.classifier = nn.Identity()  # Remove classifier\n",
    "        \n",
    "        # Fusion and classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(rgb_features + mask_features, 4)\n",
    "        )\n",
    "        \n",
    "        print(f\"RGB branch features: {rgb_features}\")\n",
    "        print(f\"Mask branch features: {mask_features}\")\n",
    "        print(f\"Total fused features: {rgb_features + mask_features}\")\n",
    "    \n",
    "    def forward(self, rgb, mask):\n",
    "        # RGB branch: color, texture, morphology\n",
    "        rgb_features = self.rgb_branch(rgb)\n",
    "        \n",
    "        # Mask branch: shape, boundaries, geometry\n",
    "        mask_features = self.mask_branch(mask)\n",
    "        \n",
    "        # Fusion: parallel paths meet at summit\n",
    "        fused_features = torch.cat([rgb_features, mask_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(fused_features)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = DualBranchEfficientNet()\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Lion optimizer (Advice 07/12)\n",
    "# Lion needs 3-10x smaller LR than Adam\n",
    "optimizer = Lion(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "print(\"\\n‚úÖ Model: Dual-Branch EfficientNetV2-S\")\n",
    "print(\"‚úÖ RGB Branch: Pretrained on ImageNet\")\n",
    "print(\"‚úÖ Mask Branch: Train from scratch\")\n",
    "print(\"‚úÖ Fusion: Late concatenation\")\n",
    "print(\"‚úÖ Optimizer: Lion (lr=3e-4)\")\n",
    "print(\"‚úÖ Scheduler: Cosine Annealing\")\n",
    "print(f\"‚úÖ Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbec7e",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6a8875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined (dual-branch).\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch with dual-branch inputs.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for images, masks, labels, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, masks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    epoch_acc = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    \n",
    "    return epoch_loss, epoch_f1, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch with dual-branch inputs.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "        for images, masks, labels, _ in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images, masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    epoch_acc = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "    \n",
    "    return epoch_loss, epoch_f1, epoch_acc, all_preds, all_targets\n",
    "\n",
    "print(\"Training functions defined (dual-branch).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "107ebd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Total epochs: 50\n",
      "  Freeze RGB branch epochs: 5\n",
      "  Early stopping patience: 10\n",
      "  RGB branch frozen for first 5 epochs (mask branch learns first)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "FREEZE_EPOCHS = 5  # Freeze RGB branch for first 5 epochs\n",
    "PATIENCE = 10\n",
    "\n",
    "# Freeze RGB branch initially (let mask branch learn)\n",
    "for param in model.rgb_branch.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Freeze RGB branch epochs: {FREEZE_EPOCHS}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")\n",
    "print(f\"  RGB branch frozen for first {FREEZE_EPOCHS} epochs (mask branch learns first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STARTED - Dual-Branch EfficientNetV2-S\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'train_loss': [], 'train_f1': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_f1': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING STARTED - Dual-Branch EfficientNetV2-S\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Unfreeze RGB branch after warmup\n",
    "    if epoch == FREEZE_EPOCHS:\n",
    "        print(f\"\\nüîì Unfreezing RGB branch at epoch {epoch}\\n\")\n",
    "        for param in model.rgb_branch.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_f1, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_f1, val_acc, val_preds, val_targets = validate_epoch(\n",
    "        model, val_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Acc: {val_acc:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_acc': val_acc,\n",
    "        }, '../best_model_dual_branch.pth')\n",
    "        print(f\"  üíæ Saved best model (F1: {val_f1:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered (no improvement for {PATIENCE} epochs)\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"Best Validation F1: {best_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac20df1",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze RGB')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss - Dual-Branch')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1].plot(history['train_f1'], label='Train F1', marker='o', color='green')\n",
    "axes[1].plot(history['val_f1'], label='Val F1', marker='s', color='orange')\n",
    "axes[1].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze RGB')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score (Macro)')\n",
    "axes[1].set_title(f'F1 Score (Best: {best_f1:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[2].plot(history['train_acc'], label='Train Acc', marker='o', color='blue')\n",
    "axes[2].plot(history['val_acc'], label='Val Acc', marker='s', color='red')\n",
    "axes[2].axvline(x=FREEZE_EPOCHS-1, color='r', linestyle='--', alpha=0.5, label='Unfreeze RGB')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Training & Validation Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../training_curves_dual_branch.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to '../training_curves_dual_branch.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint = torch.load('../best_model_dual_branch.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Get final predictions\n",
    "_, _, _, final_preds, final_targets = validate_epoch(\n",
    "    model, val_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "# Classification report\n",
    "class_names = [\"Luminal A\", \"Luminal B\", \"HER2(+)\", \"Triple Negative\"]\n",
    "print(\"\\nClassification Report - Dual-Branch EfficientNetV2-S:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(final_targets, final_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(final_targets, final_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Dual-Branch EfficientNetV2-S')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../confusion_matrix_dual_branch.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to '../confusion_matrix_dual_branch.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e29b45",
   "metadata": {},
   "source": [
    "## Step 7: Patch-Based Inference for Test Set\n",
    "\n",
    "**Advice 08/12**: \"Let the model walk the landscape step by step, preserving the original resolution.\"\n",
    "\n",
    "We train on resized images (stable), but infer using patches at full resolution (preserves details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7294ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_inference(img, mask, patch_size=384, stride=192):\n",
    "    \"\"\"Extract overlapping patches for dual-branch inference.\"\"\"\n",
    "    w, h = img.size\n",
    "    patches_img = []\n",
    "    patches_mask = []\n",
    "    \n",
    "    # Handle small images\n",
    "    if w < patch_size or h < patch_size:\n",
    "        resized_img = img.resize((patch_size, patch_size), Image.BILINEAR)\n",
    "        resized_mask = mask.resize((patch_size, patch_size), Image.NEAREST)\n",
    "        return [resized_img], [resized_mask]\n",
    "    \n",
    "    # Sliding window\n",
    "    for top in range(0, h - patch_size + 1, stride):\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch_img = img.crop((left, top, left + patch_size, top + patch_size))\n",
    "            patch_mask = mask.crop((left, top, left + patch_size, top + patch_size))\n",
    "            patches_img.append(patch_img)\n",
    "            patches_mask.append(patch_mask)\n",
    "    \n",
    "    # Handle edges\n",
    "    if (w - patch_size) % stride != 0:\n",
    "        left = w - patch_size\n",
    "        for top in range(0, h - patch_size + 1, stride):\n",
    "            patch_img = img.crop((left, top, w, top + patch_size))\n",
    "            patch_mask = mask.crop((left, top, w, top + patch_size))\n",
    "            patches_img.append(patch_img)\n",
    "            patches_mask.append(patch_mask)\n",
    "    \n",
    "    if (h - patch_size) % stride != 0:\n",
    "        top = h - patch_size\n",
    "        for left in range(0, w - patch_size + 1, stride):\n",
    "            patch_img = img.crop((left, top, left + patch_size, h))\n",
    "            patch_mask = mask.crop((left, top, left + patch_size, h))\n",
    "            patches_img.append(patch_img)\n",
    "            patches_mask.append(patch_mask)\n",
    "    \n",
    "    # Corner\n",
    "    if (w - patch_size) % stride != 0 and (h - patch_size) % stride != 0:\n",
    "        patch_img = img.crop((w - patch_size, h - patch_size, w, h))\n",
    "        patch_mask = mask.crop((w - patch_size, h - patch_size, w, h))\n",
    "        patches_img.append(patch_img)\n",
    "        patches_mask.append(patch_mask)\n",
    "    \n",
    "    return patches_img, patches_mask\n",
    "\n",
    "\n",
    "def predict_with_patches(model, img_path, mask_path, transform, patch_size=384, stride=192, device='cpu'):\n",
    "    \"\"\"Predict using patch-based inference with dual-branch model.\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    mask = Image.open(mask_path).convert('L')\n",
    "    \n",
    "    patches_img, patches_mask = extract_patches_inference(img, mask, patch_size=patch_size, stride=stride)\n",
    "    \n",
    "    all_probs = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patch_img, patch_mask in zip(patches_img, patches_mask):\n",
    "            patch_img_tensor = transform(patch_img).unsqueeze(0).to(device)\n",
    "            patch_mask_tensor = transform(patch_mask).unsqueeze(0).to(device)\n",
    "            logits = model(patch_img_tensor, patch_mask_tensor)\n",
    "            probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Average probabilities (soft voting)\n",
    "    avg_probs = np.array(all_probs).mean(axis=0)\n",
    "    prediction = avg_probs.argmax()\n",
    "    confidence = avg_probs[prediction]\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "print(\"Dual-branch patch-based inference functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873feca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Test Set with Masks\n",
    "TEST_INPUT_DIR = '../data/test_data'\n",
    "TEST_OUTPUT_DIR = '../data/test_data_preprocessed'\n",
    "\n",
    "os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(TEST_INPUT_DIR):\n",
    "    print(f\"Preprocessing test set (clean data - permissive filtering)...\")\n",
    "    print(f\"Input: {TEST_INPUT_DIR}\")\n",
    "    print(f\"Output: {TEST_OUTPUT_DIR}\")\n",
    "    print(f\"Note: Using 50% artifact threshold since test data is clean\\n\")\n",
    "    \n",
    "    test_files = sorted([f for f in os.listdir(TEST_INPUT_DIR) if f.startswith('img_')])\n",
    "    print(f\"Found {len(test_files)} test images\\n\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    failed_test_images = []\n",
    "    test_rejection_reasons = {'high_artifacts': 0, 'no_tissue': 0, 'corrupted': 0}\n",
    "    \n",
    "    for img_name in tqdm(test_files, desc=\"Preprocessing Test Set\"):\n",
    "        img_path = os.path.join(TEST_INPUT_DIR, img_name)\n",
    "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "        mask_path = os.path.join(TEST_INPUT_DIR, mask_name)\n",
    "        \n",
    "        try:\n",
    "            # Use 0.5 (50%) threshold for test - much more permissive since data is clean\n",
    "            processed_img, processed_mask, success, message = preprocess_image(\n",
    "                img_path, mask_path, max_artifact_ratio=0.7\n",
    "            )\n",
    "            \n",
    "            if success and processed_img is not None:\n",
    "                img_output_path = os.path.join(TEST_OUTPUT_DIR, img_name)\n",
    "                mask_output_path = os.path.join(TEST_OUTPUT_DIR, mask_name)\n",
    "                cv2.imwrite(img_output_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "                cv2.imwrite(mask_output_path, processed_mask)\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "                failed_test_images.append(img_name)\n",
    "                \n",
    "                if 'artifacts' in message:\n",
    "                    test_rejection_reasons['high_artifacts'] += 1\n",
    "                elif 'tissue' in message:\n",
    "                    test_rejection_reasons['no_tissue'] += 1\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            failed_test_images.append(img_name)\n",
    "            test_rejection_reasons['corrupted'] += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Set Preprocessing Complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úÖ Successful: {successful}/{len(test_files)} ({successful/len(test_files)*100:.1f}%)\")\n",
    "    print(f\"‚ùå Failed:     {failed}/{len(test_files)} ({failed/len(test_files)*100:.1f}%)\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(f\"\\nRejection breakdown:\")\n",
    "        print(f\"  üé® Too many artifacts: {test_rejection_reasons['high_artifacts']}\")\n",
    "        print(f\"  üö´ No tissue:          {test_rejection_reasons['no_tissue']}\")\n",
    "        print(f\"  üí• Corrupted:          {test_rejection_reasons['corrupted']}\")\n",
    "else:\n",
    "    print(f\"Test directory not found: {TEST_INPUT_DIR}\")\n",
    "    print(\"Skipping test set preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "TEST_DIR = '../data/test_data_preprocessed'\n",
    "\n",
    "if os.path.exists(TEST_DIR):\n",
    "    test_files = sorted([f for f in os.listdir(TEST_DIR) if f.startswith('img_')])\n",
    "    print(f\"Found {len(test_files)} test images\")\n",
    "    print(f\"Using dual-branch patch-based inference with 384√ó384 patches, stride=192 (50% overlap)\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for img_name in tqdm(test_files, desc=\"Inference\"):\n",
    "        img_path = os.path.join(TEST_DIR, img_name)\n",
    "        mask_name = img_name.replace(\"img_\", \"mask_\")\n",
    "        mask_path = os.path.join(TEST_DIR, mask_name)\n",
    "        \n",
    "        pred, conf = predict_with_patches(\n",
    "            model, img_path, mask_path, val_transform, \n",
    "            patch_size=384, stride=192, device=DEVICE\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': img_name,\n",
    "            'label': class_names[pred],\n",
    "            # 'confidence': conf\n",
    "        })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_df = pd.DataFrame(results)\n",
    "    submission_df.to_csv('../submission_dual_branch_patches.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Inference complete!\")\n",
    "    print(f\"‚úÖ Saved to: ../submission_dual_branch_patches.csv\")\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(submission_df['label'].value_counts())\n",
    "    # print(f\"\\nAverage confidence: {submission_df['confidence'].mean():.3f}\")\n",
    "else:\n",
    "    print(f\"Test directory not found: {TEST_DIR}\")\n",
    "    print(\"Please run test set preprocessing first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf5942",
   "metadata": {},
   "source": [
    "## Summary - Dual-Branch EfficientNetV2-S Pipeline\n",
    "\n",
    "### ‚úÖ All 5 Pieces of Advice Implemented:\n",
    "\n",
    "1. **Advice 05/12** (Outliers): Filtered images with >10% artifacts\n",
    "2. **Advice 04/12** (Normalization): Used batch_size=32 for BatchNorm stability\n",
    "3. **Advice 07/12** (Modern Optimizers): Lion optimizer with lr=3e-4\n",
    "4. **Advice 06/12** (Auto Augmentation): RandAugment with num_ops=2, magnitude=7\n",
    "5. **Advice 08/12** (Full Resolution): Patch-based inference at native resolution\n",
    "6. **Advice 09/12** (Masks as Focus Filters): Background removed during preprocessing\n",
    "7. **Advice 10/12** (Parallel Paths): Dual-branch architecture with late fusion\n",
    "\n",
    "### üéØ Dual-Branch Architecture:\n",
    "- **RGB Branch**: Processes color, texture, cellular morphology (21M params, pretrained)\n",
    "- **Mask Branch**: Processes shape, boundaries, spatial structure (21M params, from scratch)\n",
    "- **Fusion**: Late concatenation of 1280 + 1280 = 2560 features\n",
    "- **Total**: ~42M parameters (double single-branch)\n",
    "\n",
    "### üåâ Why Dual-Branch?\n",
    "- **Appearance vs Geometry**: RGB learns \"what\" (cell types), Mask learns \"where\" (tissue structure)\n",
    "- **Complementary Features**: Color + shape provide richer representations\n",
    "- **Robustness**: Less sensitive to staining variations (mask branch helps)\n",
    "- **Medical Imaging**: Proven effective in histopathology competitions\n",
    "\n",
    "### üìä Results:\n",
    "- Check `training_curves_dual_branch.png` for loss/F1 progression\n",
    "- Check `confusion_matrix_dual_branch.png` for per-class performance\n",
    "- Best model saved to `best_model_dual_branch.pth`\n",
    "- Test predictions saved to `submission_dual_branch_patches.csv`\n",
    "\n",
    "### üîÑ Comparison with Single-Branch:\n",
    "Run all notebooks and compare:\n",
    "- **Single EfficientNetV2-S** (21M params)\n",
    "- **Dual-Branch EfficientNetV2-S** (42M params)\n",
    "- **DenseNet-121** (8M params)\n",
    "- **Swin Transformer** (28M params)\n",
    "\n",
    "Compare:\n",
    "- Training stability and speed\n",
    "- Validation F1 scores\n",
    "- Inference confidence\n",
    "- Overfitting behavior\n",
    "\n",
    "### üí° Training Strategy:\n",
    "- **Phase 1**: Freeze RGB branch, train mask branch (first 5 epochs)\n",
    "- **Phase 2**: Unfreeze RGB branch, fine-tune both branches together\n",
    "- **Fusion**: Parallel paths meet at latent space for classification\n",
    "\n",
    "This architecture should give you the best performance - appearance + geometry = superior classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
